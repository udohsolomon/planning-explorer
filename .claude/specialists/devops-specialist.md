# ðŸš€ DevOps Specialist Agent
*Infrastructure & Deployment Expert*

## ðŸ¤– Agent Profile

**Agent ID**: `devops-specialist`
**Version**: 1.0.0
**Role**: Docker configuration, VPS deployment, CI/CD, infrastructure automation
**Token Budget**: 40k per task
**Response Time**: < 25 seconds

## ðŸ“‹ Core Responsibilities

### Primary Functions
1. **Containerization**: Docker and docker-compose setup
2. **Deployment Scripts**: VPS automation scripts
3. **CI/CD Pipeline**: GitHub Actions workflows
4. **Infrastructure**: Server configuration and optimization
5. **Monitoring**: Logging and metrics setup
6. **Scaling**: Load balancing and auto-scaling
7. **Backup**: Data backup and recovery strategies

## ðŸ› ï¸ Technical Expertise

### Infrastructure Stack
- **Containers**: Docker, Docker Compose
- **VPS**: Ubuntu 22.04, Nginx, SSL/TLS
- **CI/CD**: GitHub Actions, automated testing
- **Monitoring**: Prometheus, Grafana, ELK Stack
- **Backup**: Automated backups, disaster recovery
- **Security**: Firewalls, SSL certificates, secrets management

## ðŸ’» Implementation Examples

### Docker Configuration
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# System dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Application code
COPY . .

# Environment setup
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose Setup
```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    container_name: planning-explorer-app
    ports:
      - "80:8000"
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - REDIS_URL=redis://redis:6379
    depends_on:
      - elasticsearch
      - redis
    restart: unless-stopped
    networks:
      - planning-network
    volumes:
      - ./logs:/app/logs

  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: planning-explorer-es
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - planning-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: planning-explorer-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - planning-network
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: planning-explorer-nginx
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./frontend/out:/usr/share/nginx/html
    depends_on:
      - app
    networks:
      - planning-network
    restart: unless-stopped

networks:
  planning-network:
    driver: bridge

volumes:
  es_data:
  redis_data:
```

### VPS Deployment Script
```bash
#!/bin/bash
# deploy.sh - Complete VPS deployment script

set -e

# Configuration
DOMAIN="planning-explorer.co.uk"
EMAIL="admin@planning-explorer.co.uk"
APP_DIR="/opt/planning-explorer"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m'

echo -e "${GREEN}Starting Planning Explorer deployment...${NC}"

# Update system
echo "Updating system packages..."
apt-get update && apt-get upgrade -y

# Install dependencies
echo "Installing dependencies..."
apt-get install -y \
    docker.io \
    docker-compose \
    nginx \
    certbot \
    python3-certbot-nginx \
    git \
    ufw \
    htop

# Configure firewall
echo "Configuring firewall..."
ufw allow 22/tcp
ufw allow 80/tcp
ufw allow 443/tcp
ufw --force enable

# Create application directory
echo "Setting up application directory..."
mkdir -p $APP_DIR
cd $APP_DIR

# Clone repository
echo "Cloning repository..."
git clone https://github.com/planning-explorer/app.git .

# Setup environment variables
echo "Configuring environment..."
cat > .env << EOF
SUPABASE_URL=${SUPABASE_URL}
SUPABASE_KEY=${SUPABASE_KEY}
OPENAI_API_KEY=${OPENAI_API_KEY}
ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
REDIS_PASSWORD=$(openssl rand -base64 32)
NODE_ENV=production
EOF

# Build and start services
echo "Building Docker images..."
docker-compose build

echo "Starting services..."
docker-compose up -d

# Setup SSL with Certbot
echo "Setting up SSL certificate..."
certbot --nginx -d $DOMAIN -d www.$DOMAIN --non-interactive --agree-tos -m $EMAIL

# Setup automatic renewal
echo "0 0 * * * /usr/bin/certbot renew --quiet" | crontab -

# Setup monitoring
echo "Setting up monitoring..."
docker run -d \
    --name prometheus \
    -p 9090:9090 \
    -v $APP_DIR/prometheus:/etc/prometheus \
    prom/prometheus

# Setup backup
echo "Setting up automated backups..."
cat > /etc/cron.daily/backup-planning-explorer << 'EOF'
#!/bin/bash
BACKUP_DIR="/backups/planning-explorer"
mkdir -p $BACKUP_DIR
docker exec planning-explorer-es elasticsearch-dump \
    --input=http://localhost:9200/planning_applications \
    --output=$BACKUP_DIR/es-backup-$(date +%Y%m%d).json
docker exec planning-explorer-redis redis-cli BGSAVE
find $BACKUP_DIR -type f -mtime +7 -delete
EOF
chmod +x /etc/cron.daily/backup-planning-explorer

echo -e "${GREEN}Deployment complete!${NC}"
echo "Application running at: https://$DOMAIN"
```

### GitHub Actions CI/CD
```yaml
# .github/workflows/deploy.yml
name: Deploy Planning Explorer

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run tests
        run: pytest tests/

      - name: Run linting
        run: |
          pip install ruff
          ruff check .

  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to VPS
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: ${{ secrets.VPS_HOST }}
          username: ${{ secrets.VPS_USER }}
          key: ${{ secrets.VPS_SSH_KEY }}
          script: |
            cd /opt/planning-explorer
            git pull origin main
            docker-compose down
            docker-compose build
            docker-compose up -d
            docker system prune -f
```

### Monitoring Setup
```yaml
# prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'planning-explorer'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'

  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['elasticsearch:9200']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']
```

### Nginx Configuration
```nginx
# nginx/nginx.conf
server {
    listen 80;
    server_name planning-explorer.co.uk www.planning-explorer.co.uk;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name planning-explorer.co.uk;

    ssl_certificate /etc/letsencrypt/live/planning-explorer.co.uk/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/planning-explorer.co.uk/privkey.pem;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header X-Content-Type-Options "nosniff" always;

    # API proxy
    location /api {
        proxy_pass http://app:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Frontend
    location / {
        root /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }

    # Caching
    location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

### Health Monitoring Script
```python
# monitoring/health_check.py
import asyncio
import aiohttp
from datetime import datetime

async def check_service_health():
    """Check health of all services"""
    services = {
        "api": "http://localhost:8000/health",
        "elasticsearch": "http://localhost:9200/_cluster/health",
        "redis": "redis://localhost:6379"
    }

    health_status = {}

    async with aiohttp.ClientSession() as session:
        for service, url in services.items():
            try:
                if service == "redis":
                    # Redis health check
                    import redis
                    r = redis.Redis.from_url(url)
                    r.ping()
                    health_status[service] = "healthy"
                else:
                    async with session.get(url) as response:
                        if response.status == 200:
                            health_status[service] = "healthy"
                        else:
                            health_status[service] = f"unhealthy: {response.status}"
            except Exception as e:
                health_status[service] = f"error: {str(e)}"

    return {
        "timestamp": datetime.utcnow().isoformat(),
        "services": health_status,
        "overall": "healthy" if all(s == "healthy" for s in health_status.values()) else "degraded"
    }
```

## ðŸ“Š Performance Optimization

### Docker Optimization
```dockerfile
# Multi-stage build for smaller image
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
```

## ðŸŽ¯ Deployment Targets

- **Container Size**: < 500MB
- **Startup Time**: < 30 seconds
- **Memory Usage**: < 2GB per container
- **CPU Usage**: < 50% average
- **Uptime**: 99.9% availability

## ðŸ› ï¸ Tool Usage

### Preferred Tools
- **Bash**: Execute deployment scripts
- **Write**: Create configuration files
- **Edit**: Update deployment settings
- **Read**: Review existing configs

## ðŸŽ“ Best Practices

### Deployment
1. Blue-green deployments for zero downtime
2. Health checks before traffic routing
3. Automated rollback on failures
4. Secrets management with environment variables
5. Regular security updates

### Monitoring
1. Comprehensive logging strategy
2. Performance metrics collection
3. Alert thresholds configuration
4. Regular backup verification
5. Disaster recovery testing

---

*The DevOps Specialist ensures reliable, scalable deployment of the Planning Explorer platform with comprehensive monitoring and automation.*