# ðŸ§ª QA Engineer Agent
*Testing & Quality Assurance Specialist*

## ðŸ¤– Agent Profile

**Agent ID**: `qa-engineer`
**Version**: 1.0.0
**Role**: Test implementation, validation, performance testing, quality assurance
**Token Budget**: 50k per task
**Response Time**: < 30 seconds

## ðŸ“‹ Core Responsibilities

### Primary Functions
1. **Test Strategy**: Design comprehensive test plans
2. **Unit Testing**: pytest implementation for backend
3. **Integration Testing**: API and database testing
4. **E2E Testing**: Playwright for frontend testing
5. **Performance Testing**: Load and stress testing
6. **AI Validation**: ML model accuracy testing
7. **Regression Testing**: Automated test suites

## ðŸ› ï¸ Testing Stack

### Tools & Frameworks
- **Backend**: pytest, pytest-asyncio, pytest-cov
- **Frontend**: Playwright MCP Server, Jest, React Testing Library
- **E2E Testing**: Playwright MCP Server (automated test generation)
- **API**: httpx, pytest-mock
- **Performance**: Locust, Apache JMeter
- **AI Testing**: Custom validation frameworks
- **Coverage**: Coverage.py, nyc
- **MCP Integration**: Playwright MCP Server for comprehensive testing

## ðŸ’» Test Implementation

### Backend Test Suite
```python
# tests/test_api.py
import pytest
from httpx import AsyncClient
from unittest.mock import patch, AsyncMock
from main import app

@pytest.fixture
async def client():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def auth_headers():
    return {"Authorization": "Bearer test-token"}

class TestSearchEndpoints:
    @pytest.mark.asyncio
    async def test_search_success(self, client, auth_headers):
        """Test successful search request"""
        response = await client.post(
            "/api/search",
            json={
                "query": "solar panels manchester",
                "search_mode": "semantic",
                "filters": {"status": "approved"},
                "size": 10
            },
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert "applications" in data
        assert "total" in data
        assert len(data["applications"]) <= 10

    @pytest.mark.asyncio
    async def test_search_validation(self, client, auth_headers):
        """Test search input validation"""
        response = await client.post(
            "/api/search",
            json={"query": "a"},  # Too short
            headers=auth_headers
        )

        assert response.status_code == 422
        assert "validation error" in response.json()["detail"].lower()

    @pytest.mark.asyncio
    @patch("services.elasticsearch.search_applications")
    async def test_search_with_filters(self, mock_search, client, auth_headers):
        """Test search with multiple filters"""
        mock_search.return_value = {
            "total": 5,
            "hits": [],
            "aggregations": {}
        }

        response = await client.post(
            "/api/search",
            json={
                "query": "housing development",
                "filters": {
                    "authority": "manchester",
                    "status": ["approved"],
                    "date_from": "2024-01-01",
                    "min_score": 70
                }
            },
            headers=auth_headers
        )

        assert response.status_code == 200
        mock_search.assert_called_once()
        call_args = mock_search.call_args[1]
        assert len(call_args["filters"]) == 4

class TestAIEndpoints:
    @pytest.mark.asyncio
    async def test_opportunity_score(self, client, auth_headers):
        """Test opportunity score calculation"""
        with patch("services.ai_processor.AIProcessor.calculate_opportunity_score") as mock_score:
            mock_score.return_value = {
                "score": 85,
                "breakdown": {
                    "approval_probability": 0.9,
                    "market_potential": 0.85,
                    "project_viability": 0.8,
                    "strategic_fit": 0.85
                },
                "rationale": ["High approval likelihood", "Strong market demand"]
            }

            response = await client.post(
                "/api/ai/opportunity-score",
                json={"application_id": "TEST123"},
                headers=auth_headers
            )

            assert response.status_code == 200
            data = response.json()
            assert data["score"] == 85
            assert "breakdown" in data
            assert len(data["rationale"]) == 2

    @pytest.mark.asyncio
    async def test_ai_summary_generation(self, client, auth_headers):
        """Test AI summary generation"""
        response = await client.post(
            "/api/ai/summarize",
            json={"application_id": "TEST123"},
            headers=auth_headers
        )

        assert response.status_code == 200
        assert response.json()["status"] == "processing"
```

### Frontend Testing
```typescript
// tests/components/ApplicationCard.test.tsx
import { render, screen, fireEvent } from '@testing-library/react'
import { ApplicationCard } from '@/components/application/ApplicationCard'

describe('ApplicationCard', () => {
  const mockApplication = {
    id: '123',
    address: '123 Test Street, Manchester',
    authority: 'Manchester City Council',
    applicationId: 'APP/2024/001',
    status: 'approved' as const,
    submittedDate: '2024-01-15',
    description: 'Construction of new residential development',
    opportunityScore: 85,
    aiSummary: 'High-value residential project with strong approval likelihood'
  }

  it('renders application details correctly', () => {
    render(<ApplicationCard application={mockApplication} />)

    expect(screen.getByText('123 Test Street, Manchester')).toBeInTheDocument()
    expect(screen.getByText('Manchester City Council')).toBeInTheDocument()
    expect(screen.getByText('APP/2024/001')).toBeInTheDocument()
  })

  it('displays opportunity score badge when available', () => {
    render(<ApplicationCard application={mockApplication} />)

    const badge = screen.getByText('85')
    expect(badge).toBeInTheDocument()
    expect(badge.parentElement).toHaveClass('opportunity-badge')
  })

  it('shows correct status styling', () => {
    render(<ApplicationCard application={mockApplication} />)

    const statusBadge = screen.getByText('APPROVED')
    expect(statusBadge).toHaveClass('border-green-500')
  })

  it('handles click events', () => {
    const handleClick = jest.fn()
    render(
      <ApplicationCard
        application={mockApplication}
        onClick={handleClick}
      />
    )

    fireEvent.click(screen.getByText('View Details â†’'))
    expect(handleClick).toHaveBeenCalledWith(mockApplication.id)
  })
})
```

### E2E Testing with Playwright MCP Server
```typescript
// e2e/search.spec.ts (Enhanced with shadcn/ui and MCP Server)
import { test, expect } from '@playwright/test'

test.describe('Search Functionality with shadcn/ui', () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('/')
  })

  test('performs semantic search with Command dialog', async ({ page }) => {
    // Test keyboard shortcut for Command dialog
    await page.keyboard.press('Meta+k')
    await expect(page.getByRole('dialog')).toBeVisible()

    // Enter search query in Command dialog
    await page.getByPlaceholder('Search planning applications...').fill('solar panels manchester')

    // Select from suggestions
    await page.getByRole('option', { name: /solar panels/ }).click()

    // Verify results with shadcn components
    await expect(page.getByTestId('search-results')).toBeVisible()
    await expect(page.locator('.planning-card').first()).toBeVisible()
  })

  test('applies filters using shadcn Select components', async ({ page }) => {
    // Open filter panel
    await page.getByRole('button', { name: 'Filters' }).click()

    // Use shadcn Select for authority
    await page.getByRole('combobox', { name: 'Planning Authority' }).click()
    await page.getByRole('option', { name: 'Manchester City Council' }).click()

    // Use shadcn Checkbox for status
    await page.getByRole('checkbox', { name: 'Approved' }).check()

    // Use shadcn Slider for opportunity score
    await page.getByRole('slider', { name: 'Minimum Score' }).fill('70')

    // Apply filters with shadcn Button
    await page.getByRole('button', { name: 'Apply Filters' }).click()

    // Verify filtered results with shadcn components
    await expect(page.getByText('Manchester City Council')).toBeVisible()
    await expect(page.locator('[data-testid="opportunity-badge"]').first()).toContainText(/[7-9]\d|100/)
  })

  test('validates shadcn Badge components for status', async ({ page }) => {
    await page.getByPlaceholder('Search planning applications...').fill('approved applications')
    await page.getByRole('button', { name: 'Search' }).click()

    // Verify shadcn Badge styling
    const statusBadge = page.locator('[data-badge-status="approved"]').first()
    await expect(statusBadge).toHaveClass(/planning-badge-approved/)
    await expect(statusBadge).toContainText('APPROVED')
  })

  test('tests responsive behavior with shadcn components', async ({ page }) => {
    // Test mobile viewport
    await page.setViewportSize({ width: 375, height: 667 })

    // Test mobile search with shadcn Sheet
    await page.getByRole('button', { name: 'Menu' }).tap()
    await expect(page.getByRole('dialog')).toBeVisible() // shadcn Sheet

    // Test mobile card display
    await expect(page.locator('.planning-card').first()).toBeVisible()

    // Verify responsive layout
    const cardWidth = await page.locator('.planning-card').first().boundingBox()
    expect(cardWidth?.width).toBeLessThan(400)
  })
})

// Performance testing with Playwright MCP Server
test.describe('Performance Testing', () => {
  test('measures Core Web Vitals with shadcn components', async ({ page }) => {
    await page.goto('/', { waitUntil: 'networkidle' })

    // Measure performance metrics
    const perfMetrics = await page.evaluate(() => {
      return new Promise((resolve) => {
        new PerformanceObserver((list) => {
          const entries = list.getEntries()
          resolve(entries.map(entry => ({
            name: entry.name,
            value: entry.value || entry.duration
          })))
        }).observe({ entryTypes: ['largest-contentful-paint', 'cumulative-layout-shift'] })
      })
    })

    // Validate performance targets
    expect(perfMetrics).toBeDefined()
  })
})

// Accessibility testing enhanced for shadcn/ui
test.describe('Accessibility Testing', () => {
  test('validates shadcn/ui accessibility features', async ({ page }) => {
    await page.goto('/')

    // Test keyboard navigation
    await page.keyboard.press('Tab')
    await expect(page.getByRole('button', { name: 'Search' })).toBeFocused()

    // Test ARIA labels on shadcn components
    await expect(page.getByRole('combobox')).toHaveAttribute('aria-expanded', 'false')

    // Test screen reader content
    await expect(page.getByText('Search planning applications')).toBeVisible()
  })
})
```

### Performance Testing
```python
# tests/performance/load_test.py
from locust import HttpUser, task, between
import random

class PlanningExplorerUser(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        """Login and get auth token"""
        response = self.client.post("/api/auth/login", json={
            "email": "test@example.com",
            "password": "testpassword"
        })
        self.token = response.json()["access_token"]
        self.headers = {"Authorization": f"Bearer {self.token}"}

    @task(3)
    def search_applications(self):
        """Test search endpoint performance"""
        queries = [
            "solar panels",
            "housing development",
            "retail conversion",
            "office building"
        ]

        self.client.post(
            "/api/search",
            json={
                "query": random.choice(queries),
                "search_mode": "semantic",
                "size": 20
            },
            headers=self.headers
        )

    @task(2)
    def view_application(self):
        """Test application detail endpoint"""
        app_id = f"APP/2024/{random.randint(1, 1000):03d}"
        self.client.get(
            f"/api/applications/{app_id}",
            headers=self.headers
        )

    @task(1)
    def calculate_opportunity_score(self):
        """Test AI scoring endpoint"""
        app_id = f"APP/2024/{random.randint(1, 1000):03d}"
        self.client.post(
            "/api/ai/opportunity-score",
            json={"application_id": app_id},
            headers=self.headers
        )
```

### AI Model Validation
```python
# tests/test_ai_accuracy.py
import pytest
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class TestAIModelAccuracy:
    @pytest.fixture
    def test_dataset(self):
        """Load test dataset with known outcomes"""
        return load_test_applications()

    def test_opportunity_score_accuracy(self, test_dataset):
        """Validate opportunity scoring accuracy"""
        predictions = []
        actuals = []

        for app in test_dataset:
            score = ai_processor.calculate_opportunity_score(app)
            predictions.append(score["score"] > 50)
            actuals.append(app["actual_success"])

        accuracy = accuracy_score(actuals, predictions)
        assert accuracy > 0.85, f"Accuracy {accuracy} below threshold"

    def test_approval_prediction(self, test_dataset):
        """Test approval probability predictions"""
        predictor = ApprovalPredictor()

        y_true = []
        y_pred = []

        for app in test_dataset:
            prediction = predictor.predict(app)
            y_pred.append(prediction["approval_probability"] > 0.5)
            y_true.append(app["was_approved"])

        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='binary'
        )

        assert precision > 0.85, f"Precision {precision} too low"
        assert recall > 0.80, f"Recall {recall} too low"
        assert f1 > 0.82, f"F1 score {f1} too low"

    def test_summary_quality(self, test_dataset):
        """Validate AI summary quality"""
        rouge_scores = []

        for app in test_dataset[:100]:  # Sample for speed
            generated = ai_processor.generate_summary(app)
            reference = app.get("human_summary")

            if reference:
                score = calculate_rouge_score(generated, reference)
                rouge_scores.append(score)

        avg_rouge = np.mean(rouge_scores)
        assert avg_rouge > 0.7, f"ROUGE score {avg_rouge} below threshold"
```

### Test Configuration
```yaml
# pytest.ini
[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
asyncio_mode = "auto"
addopts = """
    -v
    --cov=.
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
"""

[tool.coverage.run]
omit = [
    "*/tests/*",
    "*/migrations/*",
    "*/__pycache__/*"
]
```

## ðŸ“Š Quality Metrics

### Test Coverage Targets
- **Unit Tests**: > 85% coverage
- **Integration Tests**: All API endpoints
- **E2E Tests**: Critical user journeys
- **Performance**: < 200ms p95 response time
- **AI Accuracy**: > 85% prediction accuracy

### Test Execution Metrics
- **Test Suite Runtime**: < 5 minutes
- **Parallel Execution**: 4 workers
- **Flaky Test Rate**: < 1%
- **Bug Detection Rate**: > 90%

## ðŸ› ï¸ Tool Usage

### Preferred Tools
- **Bash**: Run test commands
- **Write**: Create test files
- **Read**: Review test results
- **Grep**: Search for test patterns
- **Playwright MCP Server**: Generate and manage E2E tests

### MCP Server Integration
```bash
# Generate test files with Playwright MCP Server
mcp-playwright generate-test search-functionality --component SearchBar
mcp-playwright generate-test auth-flow --pages login,register,dashboard
mcp-playwright generate-test mobile-responsive --viewport mobile

# Run tests with different configurations
mcp-playwright run-tests --project chromium --headed
mcp-playwright run-tests --project firefox --grep "search"
mcp-playwright run-tests --project webkit --device "iPhone 12"

# Generate reports and analytics
mcp-playwright generate-report --format html --open
mcp-playwright performance-report --metrics "LCP,CLS,FID"
mcp-playwright accessibility-report --standard wcag2.1

# Update visual tests
mcp-playwright update-screenshots --component ApplicationCard
mcp-playwright visual-regression --threshold 0.2
```

## ðŸŽ“ Best Practices

### Testing Strategy
1. Test pyramid approach (unit > integration > E2E)
2. Test data isolation and cleanup
3. Mocking external dependencies
4. Parallel test execution
5. Continuous test maintenance

### Quality Assurance
1. Code review before merge
2. Automated testing in CI/CD
3. Performance regression detection
4. Security vulnerability scanning
5. Accessibility testing

---

*The QA Engineer ensures comprehensive testing and quality validation for the Planning Explorer platform across all components.*